#!/usr/bin/env python3
"""ResumeMate ä½¿ç”¨è€…é«”é©—æ¸¬è©¦è…³æœ¬"""

import asyncio
import sys
import os
from typing import List, Dict, Any

sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", "src"))

from backend.models import Question
from backend.processor import ResumeMateProcessor


class UXTestRunner:
    """UXæ¸¬è©¦åŸ·è¡Œå™¨"""

    def __init__(self):
        self.processor = ResumeMateProcessor()
        self.test_results = []

    async def test_common_questions(self) -> Dict[str, Any]:
        """æ¸¬è©¦å¸¸è¦‹å•é¡Œçš„ä½¿ç”¨è€…é«”é©—"""
        print("ğŸ“ æ¸¬è©¦å¸¸è¦‹å±¥æ­·å•é¡Œ...")

        common_questions = [
            ("è‡ªæˆ‘ä»‹ç´¹", "ä»‹ç´¹ä¸€ä¸‹è‡ªå·±"),
            ("æŠ€èƒ½æŸ¥è©¢", "ä½ æœ‰ä»€éº¼æŠ€èƒ½ï¼Ÿ"),
            ("å·¥ä½œç¶“é©—", "ä½ çš„å·¥ä½œç¶“é©—å¦‚ä½•ï¼Ÿ"),
            ("å°ˆæ¡ˆç¶“æ­·", "å‘Šè¨´æˆ‘ä½ åšéçš„å°ˆæ¡ˆ"),
            ("è¯çµ¡æ–¹å¼", "å¦‚ä½•è¯çµ¡ä½ ï¼Ÿ"),
            ("æ•™è‚²èƒŒæ™¯", "ä½ çš„å­¸æ­·æ˜¯ä»€éº¼ï¼Ÿ"),
        ]

        results = []
        for category, question in common_questions:
            print(f"  æ¸¬è©¦ [{category}]: {question}")

            q = Question(text=question)
            response = await self.processor.process_question(q)

            # è©•ä¼°å›ç­”å“è³ª
            quality_score = self._evaluate_answer_quality(response.answer)

            result = {
                "category": category,
                "question": question,
                "answer_length": len(response.answer),
                "confidence": response.confidence,
                "quality_score": quality_score,
                "sources_count": len(response.sources),
                "has_action": response.action is not None,
            }

            results.append(result)
            print(
                f"    âœ… å›ç­”é•·åº¦: {result['answer_length']}, ä¿¡å¿ƒ: {result['confidence']:.3f}, å“è³ª: {result['quality_score']:.2f}"
            )

        return {
            "test_name": "common_questions",
            "results": results,
            "avg_confidence": sum(r["confidence"] for r in results) / len(results),
            "avg_quality": sum(r["quality_score"] for r in results) / len(results),
        }

    async def test_edge_cases(self) -> Dict[str, Any]:
        """æ¸¬è©¦é‚Šç•Œæƒ…æ³"""
        print("ğŸ” æ¸¬è©¦é‚Šç•Œæƒ…æ³...")

        edge_cases = [
            ("ç©ºå•é¡Œ", ""),
            ("è¶…çŸ­å•é¡Œ", "ä½ å¥½"),
            ("é‡è¤‡å•é¡Œ", "ä½ ä½ ä½ æ˜¯æ˜¯èª°èª°ï¼Ÿ"),
            ("æ··åˆèªè¨€", "Hello, ä½ å¥½å—ï¼Ÿ How are you?"),
            ("éå±¥æ­·ç›¸é—œ", "ä»Šå¤©å¤©æ°£å¦‚ä½•ï¼Ÿ"),
            ("æ¨¡ç³Šå•é¡Œ", "é‚£å€‹é‚£å€‹...å—¯..."),
        ]

        results = []
        for category, question in edge_cases:
            print(f"  æ¸¬è©¦ [{category}]: '{question}'")

            try:
                q = Question(text=question)
                response = await self.processor.process_question(q)

                result = {
                    "category": category,
                    "question": question,
                    "success": True,
                    "answer_length": len(response.answer),
                    "confidence": response.confidence,
                    "has_graceful_handling": self._check_graceful_handling(
                        response.answer
                    ),
                }

                print(f"    âœ… è™•ç†æˆåŠŸ, ä¿¡å¿ƒ: {result['confidence']:.3f}")

            except Exception as e:
                result = {
                    "category": category,
                    "question": question,
                    "success": False,
                    "error": str(e),
                    "has_graceful_handling": False,
                }
                print(f"    âŒ è™•ç†å¤±æ•—: {e}")

            results.append(result)

        success_rate = sum(1 for r in results if r["success"]) / len(results)

        return {
            "test_name": "edge_cases",
            "results": results,
            "success_rate": success_rate,
        }

    async def test_response_consistency(self) -> Dict[str, Any]:
        """æ¸¬è©¦å›æ‡‰ä¸€è‡´æ€§"""
        print("ğŸ”„ æ¸¬è©¦å›æ‡‰ä¸€è‡´æ€§...")

        test_question = "ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±"
        responses = []

        # å¤šæ¬¡è©¢å•åŒä¸€å•é¡Œ
        for i in range(3):
            print(f"  ç¬¬ {i+1} æ¬¡æ¸¬è©¦...")
            q = Question(text=test_question)
            response = await self.processor.process_question(q)
            responses.append(response)

        # åˆ†æä¸€è‡´æ€§
        confidences = [r.confidence for r in responses]
        answer_lengths = [len(r.answer) for r in responses]

        confidence_variance = self._calculate_variance(confidences)
        length_variance = self._calculate_variance(answer_lengths)

        return {
            "test_name": "response_consistency",
            "responses_count": len(responses),
            "confidence_variance": confidence_variance,
            "length_variance": length_variance,
            "avg_confidence": sum(confidences) / len(confidences),
            "avg_length": sum(answer_lengths) / len(answer_lengths),
        }

    def _evaluate_answer_quality(self, answer: str) -> float:
        """è©•ä¼°å›ç­”å“è³ªï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        score = 0.0

        # é•·åº¦æª¢æŸ¥ (20%)
        if len(answer) > 100:
            score += 0.2
        elif len(answer) > 50:
            score += 0.1

        # å…§å®¹å®Œæ•´æ€§æª¢æŸ¥ (30%)
        keywords = ["éŸ“ä¸–ç¿”", "æŠ€èƒ½", "ç¶“é©—", "å°ˆæ¡ˆ", "å·¥ä½œ", "Python", "AI"]
        found_keywords = sum(1 for kw in keywords if kw in answer)
        score += (found_keywords / len(keywords)) * 0.3

        # å°ˆæ¥­æ€§æª¢æŸ¥ (25%)
        professional_terms = ["é–‹ç™¼", "æŠ€è¡“", "ç³»çµ±", "è¨­è¨ˆ", "å¯¦ä½œ", "å„ªåŒ–"]
        found_terms = sum(1 for term in professional_terms if term in answer)
        score += (found_terms / len(professional_terms)) * 0.25

        # çµæ§‹æ€§æª¢æŸ¥ (25%)
        if "ã€‚" in answer or "ï¼Œ" in answer:  # æœ‰æ¨™é»ç¬¦è™Ÿ
            score += 0.15
        if len(answer.split()) > 10:  # æœ‰è¶³å¤ çš„è©å½™
            score += 0.1

        return min(score, 1.0)

    def _check_graceful_handling(self, answer: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰å„ªé›…çš„éŒ¯èª¤è™•ç†"""
        graceful_indicators = ["å¾ˆæŠ±æ­‰", "ç„¡æ³•", "ä¸æ¸…æ¥š", "è«‹", "å¯ä»¥", "å»ºè­°", "å¹«åŠ©"]
        return any(indicator in answer for indicator in graceful_indicators)

    def _calculate_variance(self, values: List[float]) -> float:
        """è¨ˆç®—è®Šç•°æ•¸"""
        if len(values) < 2:
            return 0.0

        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance

    async def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰UXæ¸¬è©¦"""
        print("ğŸš€ ResumeMate UX æ¸¬è©¦é–‹å§‹")
        print("=" * 60)

        print("æ­£åœ¨åˆå§‹åŒ–...")

        # åŸ·è¡Œå„é …æ¸¬è©¦
        common_test = await self.test_common_questions()
        print()

        edge_test = await self.test_edge_cases()
        print()

        consistency_test = await self.test_response_consistency()
        print()

        # æ•´åˆçµæœ
        print("ğŸ“Š UX æ¸¬è©¦å ±å‘Š")
        print("=" * 60)

        print("ğŸ“ å¸¸è¦‹å•é¡Œæ¸¬è©¦:")
        print(f"  å¹³å‡ä¿¡å¿ƒåˆ†æ•¸: {common_test['avg_confidence']:.3f}")
        print(f"  å¹³å‡å“è³ªåˆ†æ•¸: {common_test['avg_quality']:.3f}")

        print("\nğŸ” é‚Šç•Œæƒ…æ³æ¸¬è©¦:")
        print(f"  æˆåŠŸè™•ç†ç‡: {edge_test['success_rate']*100:.1f}%")

        print("\nğŸ”„ å›æ‡‰ä¸€è‡´æ€§æ¸¬è©¦:")
        print(f"  ä¿¡å¿ƒåˆ†æ•¸è®Šç•°æ•¸: {consistency_test['confidence_variance']:.4f}")
        print(f"  å›ç­”é•·åº¦è®Šç•°æ•¸: {consistency_test['length_variance']:.1f}")

        # æ•´é«”è©•ç´š
        overall_score = (
            common_test["avg_confidence"] * 0.4
            + common_test["avg_quality"] * 0.3
            + edge_test["success_rate"] * 0.2
            + (1 - min(consistency_test["confidence_variance"], 1.0)) * 0.1
        )

        if overall_score >= 0.8:
            grade = "ğŸ† å„ªç§€"
        elif overall_score >= 0.7:
            grade = "âœ… è‰¯å¥½"
        elif overall_score >= 0.6:
            grade = "âš ï¸ æ™®é€š"
        else:
            grade = "âŒ éœ€è¦æ”¹å–„"

        print(f"\nğŸ¯ æ•´é«”UXè©•ç´š: {grade}")
        print(f"   ç¶œåˆåˆ†æ•¸: {overall_score:.3f}")

        print("\nâœ… UXæ¸¬è©¦å®Œæˆ")

        return {
            "common_questions": common_test,
            "edge_cases": edge_test,
            "consistency": consistency_test,
            "overall_score": overall_score,
            "grade": grade,
        }


async def main():
    """ä¸»å‡½æ•¸"""
    tester = UXTestRunner()
    results = await tester.run_all_tests()
    return results


if __name__ == "__main__":
    asyncio.run(main())
