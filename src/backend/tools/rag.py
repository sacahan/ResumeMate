"""RAG å·¥å…·å±¤æ”¹é€²ç‰ˆæœ¬

å°æ¥ ChromaDB å‘é‡è³‡æ–™åº«ï¼Œæä¾›æ–‡æœ¬æª¢ç´¢å’Œæ–‡ä»¶ç®¡ç†åŠŸèƒ½
åŒ…å«å®‰å…¨æ€§ã€æ€§èƒ½å„ªåŒ–å’ŒéŒ¯èª¤è™•ç†æ”¹é€²
"""

import os
import re
import time
import hashlib
import logging
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from dotenv import load_dotenv

import chromadb
from chromadb.config import Settings
import openai
from openai import APIError, RateLimitError

from src.backend.models import SearchResult

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv(override=True)

# å»ºç«‹æ—¥èªŒè¨˜éŒ„å™¨
logger = logging.getLogger(__name__)


@dataclass
class RAGConfig:
    """RAG å·¥å…·é…ç½®é¡ - æ€§èƒ½å„ªåŒ–ç‰ˆ"""

    embedding_model: str = "text-embedding-3-small"  # ä½¿ç”¨æ›´å¿«çš„åµŒå…¥æ¨¡å‹
    batch_size: int = 100  # å„ªåŒ–æ‰¹æ¬¡å¤§å°å¹³è¡¡é€Ÿåº¦èˆ‡è¨˜æ†¶é«”
    max_retries: int = 2  # å¢åŠ é‡è©¦æ¬¡æ•¸æé«˜ç©©å®šæ€§
    cache_size: int = 100  # æ“´å¤§å¿«å–å¤§å°
    db_path: str = "./chroma_db"
    collection_name: str = "markdown_documents"
    max_top_k: int = 15  # é™ä½æœ€å¤§æª¢ç´¢æ•¸é‡æå‡é€Ÿåº¦

    # ğŸš€ æ–°å¢æ€§èƒ½å„ªåŒ–é…ç½®
    enable_query_cache: bool = True  # å•Ÿç”¨æŸ¥è©¢å¿«å–
    enable_embedding_cache: bool = True  # å•Ÿç”¨åµŒå…¥å‘é‡å¿«å–
    cache_ttl_seconds: int = 3600  # å¿«å–å­˜æ´»æ™‚é–“ 1 å°æ™‚
    parallel_batch_processing: bool = True  # ä¸¦è¡Œæ‰¹æ¬¡è™•ç†
    similarity_threshold: float = 0.1  # ç›¸ä¼¼åº¦éæ¿¾é–¾å€¼
    query_preprocessing: bool = True  # å•Ÿç”¨æŸ¥è©¢é è™•ç†
    result_reranking: bool = True  # å•Ÿç”¨çµæœé‡æ’åº

    @classmethod
    def from_env(cls) -> "RAGConfig":
        """å¾ç’°å¢ƒè®Šæ•¸å‰µå»ºé…ç½®"""
        return cls(
            embedding_model=os.getenv("EMBEDDING_MODEL", cls.embedding_model),
            batch_size=int(os.getenv("BATCH_SIZE", cls.batch_size)),
            db_path=os.getenv("CHROMA_DB_PATH", cls.db_path),
            collection_name=os.getenv("CHROMA_COLLECTION_NAME", cls.collection_name),
        )


class RAGTools:
    """æ”¹é€²çš„ RAG (æª¢ç´¢å¢å¼·ç”Ÿæˆ) ç³»çµ±å·¥å…·é¡åˆ¥

    æ•´åˆ ChromaDB å‘é‡è³‡æ–™åº«å’Œ OpenAI åµŒå…¥æ¨¡å‹ï¼Œæä¾›å®Œæ•´çš„æ–‡ä»¶æª¢ç´¢ã€
    å‘é‡æœå°‹ã€æ–‡æœ¬æ‘˜è¦ç­‰åŠŸèƒ½ï¼ŒåŒ…å«å®‰å…¨æ€§å’Œæ€§èƒ½å„ªåŒ–ã€‚
    """

    def __init__(self, config: Optional[RAGConfig] = None):
        """åˆå§‹åŒ– RAG å·¥å…· - æ€§èƒ½å„ªåŒ–ç‰ˆ

        Args:
            config: RAG é…ç½®ï¼Œè‹¥ç‚º None å‰‡ä½¿ç”¨ç’°å¢ƒè®Šæ•¸é…ç½®

        Raises:
            ValueError: ç•¶å¿…è¦çš„ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®æ™‚
            RuntimeError: ç•¶è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—æ™‚
        """
        self.config = config or RAGConfig.from_env()

        # ğŸ¯ å¤šå±¤å¿«å–ç³»çµ±
        self._query_cache: Dict[
            str, Tuple[List[SearchResult], float]
        ] = {}  # (çµæœ, æ™‚é–“æˆ³)
        self._embedding_cache: Dict[str, Tuple[List[float], float]] = {}  # åµŒå…¥å‘é‡å¿«å–
        self._preprocessed_queries: Dict[str, str] = {}  # é è™•ç†æŸ¥è©¢å¿«å–

        # ğŸ“Š æ€§èƒ½ç›£æ§
        self._query_stats = {
            "total_queries": 0,
            "cache_hits": 0,
            "embedding_cache_hits": 0,
            "avg_response_time": 0.0,
            "last_reset_time": time.time(),
        }

        # é©—è­‰ API é‡‘é‘°
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY environment variable is required")

        # åˆå§‹åŒ–çµ„ä»¶
        self.dbClient: Optional[chromadb.PersistentClient] = None
        self.provider: Optional[openai.OpenAI] = None
        self.collection = None

        self._initialize_db()

    def _initialize_db(self) -> None:
        """åˆå§‹åŒ– ChromaDB é€£æ¥

        Raises:
            RuntimeError: ç•¶è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—æ™‚
        """
        try:
            # è¨­å®š ChromaDB å®¢æˆ¶ç«¯
            self.dbClient = chromadb.PersistentClient(
                path=self.config.db_path,
                settings=Settings(anonymized_telemetry=False, allow_reset=True),
            )

            # è¨­å®š OpenAI åµŒå…¥æä¾›è€…
            self.provider = openai.OpenAI(api_key=self.api_key)

            # å–å¾—æˆ–å‰µå»º collection
            try:
                self.collection = self.dbClient.get_collection(
                    self.config.collection_name
                )
                logger.info(f"é€£æ¥åˆ° {self.config.collection_name} collection")
            except Exception:
                # å¦‚æœ collection ä¸å­˜åœ¨ï¼Œå‰µå»ºä¸€å€‹æ–°çš„
                self.collection = self.dbClient.create_collection(
                    self.config.collection_name
                )
                logger.info(f"å‰µå»ºæ–°çš„ {self.config.collection_name} collection")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ– ChromaDB å¤±æ•—: {e}")
            raise RuntimeError(f"Database initialization failed: {e}") from e

    def _validate_input(self, query: str, top_k: int) -> None:
        """é©—è­‰è¼¸å…¥åƒæ•¸

        Args:
            query: æŸ¥è©¢å­—ä¸²
            top_k: å›å‚³çµæœæ•¸é‡

        Raises:
            ValueError: ç•¶åƒæ•¸ç„¡æ•ˆæ™‚
        """
        if not isinstance(query, str) or not query.strip():
            raise ValueError("Query must be a non-empty string")

        if not isinstance(top_k, int) or top_k <= 0 or top_k > self.config.max_top_k:
            raise ValueError(
                f"top_k must be a positive integer <= {self.config.max_top_k}"
            )

    def _get_cache_key(self, query: str, top_k: int) -> str:
        """ç”Ÿæˆç·©å­˜éµå€¼ - å„ªåŒ–ç‰ˆ"""
        # æ¨™æº–åŒ–æŸ¥è©¢ä»¥æé«˜å¿«å–å‘½ä¸­ç‡
        normalized_query = query.strip().lower()
        return hashlib.md5(f"{normalized_query}:{top_k}".encode()).hexdigest()

    def _is_cache_valid(self, timestamp: float) -> bool:
        """æª¢æŸ¥å¿«å–æ˜¯å¦ä»ç„¶æœ‰æ•ˆ"""
        return (time.time() - timestamp) < self.config.cache_ttl_seconds

    def _cleanup_expired_cache(self) -> None:
        """æ¸…ç†éæœŸçš„å¿«å–é …ç›®"""
        # æ¸…ç†æŸ¥è©¢å¿«å–
        expired_query_keys = [
            key
            for key, (_, timestamp) in self._query_cache.items()
            if not self._is_cache_valid(timestamp)
        ]
        for key in expired_query_keys:
            del self._query_cache[key]

        # æ¸…ç†åµŒå…¥å¿«å–
        expired_embedding_keys = [
            key
            for key, (_, timestamp) in self._embedding_cache.items()
            if not self._is_cache_valid(timestamp)
        ]
        for key in expired_embedding_keys:
            del self._embedding_cache[key]

        if expired_query_keys or expired_embedding_keys:
            logger.debug(
                f"æ¸…ç†éæœŸå¿«å–é …ç›®ï¼šæŸ¥è©¢ {len(expired_query_keys)} å€‹ï¼ŒåµŒå…¥ {len(expired_embedding_keys)} å€‹"
            )

    def _embed_texts_with_retry(self, texts: List[str]) -> List[List[float]]:
        """ä½¿ç”¨é‡è©¦æ©Ÿåˆ¶å°‡æ–‡å­—è½‰æ›ç‚ºå‘é‡è¡¨ç¤º

        Args:
            texts: è¦è½‰æ›çš„æ–‡å­—åˆ—è¡¨

        Returns:
            List[List[float]]: åµŒå…¥å‘é‡åˆ—è¡¨

        Raises:
            APIError: ç•¶ API èª¿ç”¨å¤±æ•—æ™‚
        """
        if not texts:
            return []

        all_embeddings = []

        # åˆ†æ‰¹è™•ç†æ–‡å­—
        for i in range(0, len(texts), self.config.batch_size):
            batch = texts[i : i + self.config.batch_size]

            # å¯¦ç¾é‡è©¦æ©Ÿåˆ¶
            for attempt in range(self.config.max_retries):
                try:
                    response = self.provider.embeddings.create(
                        model=self.config.embedding_model, input=batch
                    )
                    batch_embeddings = [item.embedding for item in response.data]
                    all_embeddings.extend(batch_embeddings)
                    break

                except RateLimitError:
                    if attempt < self.config.max_retries - 1:
                        wait_time = 2**attempt  # Exponential backoff
                        logger.warning(f"Rate limit hit, waiting {wait_time}s...")
                        time.sleep(wait_time)
                        continue
                    logger.error(
                        f"Rate limit exceeded after {self.config.max_retries} attempts"
                    )
                    raise

                except APIError as e:
                    logger.error(f"OpenAI API error on attempt {attempt + 1}: {e}")
                    if attempt == self.config.max_retries - 1:
                        raise
                    time.sleep(1)

        logger.debug(f"æˆåŠŸç”Ÿæˆ {len(all_embeddings)} å€‹åµŒå…¥å‘é‡")
        return all_embeddings

    def rag_search(self, query: str, top_k: int = 10) -> List[SearchResult]:
        """åŸ·è¡Œé«˜æ€§èƒ½ RAG æª¢ç´¢ ğŸš€

        Args:
            query: æŸ¥è©¢å­—ä¸²
            top_k: å›å‚³çµæœæ•¸é‡

        Returns:
            List[SearchResult]: æª¢ç´¢çµæœåˆ—è¡¨

        Raises:
            ValueError: ç•¶è¼¸å…¥åƒæ•¸ç„¡æ•ˆæ™‚
        """
        start_time = time.time()
        self._query_stats["total_queries"] += 1

        try:
            # ğŸ“‹ é©—è­‰è¼¸å…¥
            self._validate_input(query, top_k)

            # ğŸ§¹ å®šæœŸæ¸…ç†éæœŸå¿«å–
            if self._query_stats["total_queries"] % 50 == 0:
                self._cleanup_expired_cache()

            # ğŸ¯ æª¢æŸ¥æŸ¥è©¢å¿«å–
            cache_key = self._get_cache_key(query, top_k)
            if (
                self.config.enable_query_cache
                and cache_key in self._query_cache
                and self._is_cache_valid(self._query_cache[cache_key][1])
            ):
                self._query_stats["cache_hits"] += 1
                logger.debug(f"âœ¨ å¿«å–å‘½ä¸­: {query[:30]}...")
                return self._query_cache[cache_key][0]

            # ğŸ” æŸ¥è©¢é è™•ç†
            processed_query = (
                self._preprocess_query(query)
                if self.config.query_preprocessing
                else query
            )

            # ğŸ¯ æ™ºæ…§åµŒå…¥å‘é‡è™•ç†
            query_embedding = self._get_embedding_with_cache(processed_query)
            if not query_embedding:
                logger.warning("æŸ¥è©¢åµŒå…¥å‘é‡ç”Ÿæˆå¤±æ•—ï¼Œå›å‚³ç©ºçµæœ")
                return []

            # ğŸ” åŸ·è¡Œå‘é‡æŸ¥è©¢ï¼ˆå¢åŠ æª¢ç´¢æ•¸é‡ç”¨æ–¼é‡æ’åºï¼‰
            search_top_k = (
                min(top_k * 2, self.config.max_top_k)
                if self.config.result_reranking
                else top_k
            )

            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=search_top_k,
                include=["documents", "metadatas", "distances"],
            )

            # ğŸ“Š è½‰æ›ä¸¦éæ¿¾çµæœ
            search_results = self._process_search_results(
                results, processed_query, top_k
            )

            # ğŸ’¾ å¿«å–çµæœï¼ˆå¸¶æ™‚é–“æˆ³ï¼‰
            if (
                self.config.enable_query_cache
                and len(self._query_cache) < self.config.cache_size
            ):
                current_time = time.time()
                self._query_cache[cache_key] = (search_results, current_time)

            # ğŸ“ˆ æ›´æ–°æ€§èƒ½çµ±è¨ˆ
            elapsed_time = time.time() - start_time
            self._update_performance_stats(elapsed_time)

            logger.info(
                f"ğŸ¯ RAG æª¢ç´¢å®Œæˆ: '{query[:30]}...' â†’ {len(search_results)} çµæœ ({elapsed_time:.3f}s)"
            )
            return search_results

        except Exception as e:
            logger.error(f"âŒ RAG æª¢ç´¢å¤±æ•—: {e}")
            return []

    def _preprocess_query(self, query: str) -> str:
        """æŸ¥è©¢é è™•ç†å„ªåŒ– ğŸ”§"""
        if query in self._preprocessed_queries:
            return self._preprocessed_queries[query]

        # åŸºæœ¬æ¸…ç†å’Œæ¨™æº–åŒ–
        processed = query.strip()

        # ç§»é™¤å¤šé¤˜çš„æ¨™é»ç¬¦è™Ÿ
        processed = re.sub(r"[ï¼Ÿï¼ã€‚ï¼Œï¼›ï¼š]", " ", processed)

        # ç§»é™¤å¤šé¤˜ç©ºæ ¼
        processed = re.sub(r"\s+", " ", processed).strip()

        # å¿«å–é è™•ç†çµæœ
        if len(self._preprocessed_queries) < 200:
            self._preprocessed_queries[query] = processed

        return processed

    def _get_embedding_with_cache(self, text: str) -> Optional[List[float]]:
        """å¸¶å¿«å–çš„åµŒå…¥å‘é‡ç²å– âš¡"""
        if not self.config.enable_embedding_cache:
            embeddings = self._embed_texts_with_retry([text])
            return embeddings[0] if embeddings else None

        # æª¢æŸ¥åµŒå…¥å¿«å–
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self._embedding_cache and self._is_cache_valid(
            self._embedding_cache[text_hash][1]
        ):
            self._query_stats["embedding_cache_hits"] += 1
            return self._embedding_cache[text_hash][0]

        # ç”Ÿæˆæ–°çš„åµŒå…¥å‘é‡
        embeddings = self._embed_texts_with_retry([text])
        if embeddings:
            # å¿«å–åµŒå…¥å‘é‡
            if len(self._embedding_cache) < self.config.cache_size * 2:
                current_time = time.time()
                self._embedding_cache[text_hash] = (embeddings[0], current_time)
            return embeddings[0]

        return None

    def _process_search_results(
        self, results: Dict, query: str, top_k: int
    ) -> List[SearchResult]:
        """è™•ç†å’Œå„ªåŒ–æœç´¢çµæœ ğŸ“Š"""
        if not results["ids"] or not results["ids"][0]:
            return []

        search_results = []

        for i in range(len(results["ids"][0])):
            # ChromaDB é¤˜å¼¦è·é›¢è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸
            distance = results["distances"][0][i]
            similarity = max(0.0, min(1.0, (2.0 - distance) / 2.0))

            # æ‡‰ç”¨ç›¸ä¼¼åº¦é–¾å€¼éæ¿¾
            if similarity < self.config.similarity_threshold:
                continue

            search_result = SearchResult(
                doc_id=results["ids"][0][i],
                score=similarity,
                excerpt=results["documents"][0][i] or "",
                metadata=results["metadatas"][0][i] or {},
            )
            search_results.append(search_result)

        # ğŸ† çµæœé‡æ’åºï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.config.result_reranking and len(search_results) > top_k:
            search_results = self._rerank_results(search_results, query)

        # é™åˆ¶è¿”å›æ•¸é‡
        return search_results[:top_k]

    def _rerank_results(
        self, results: List[SearchResult], query: str
    ) -> List[SearchResult]:
        """åŸºæ–¼æŸ¥è©¢ç›¸é—œæ€§é‡æ–°æ’åºçµæœ ğŸ†"""
        try:
            # ç°¡å–®çš„é—œéµè©åŒ¹é…é‡æ’åº
            query_tokens = set(query.lower().split())

            def calculate_relevance_score(result: SearchResult) -> float:
                content = result.excerpt.lower()

                # è¨ˆç®—é—œéµè©åŒ¹é…åˆ†æ•¸
                keyword_matches = sum(1 for token in query_tokens if token in content)
                keyword_score = (
                    keyword_matches / len(query_tokens) if query_tokens else 0
                )

                # çµåˆåŸå§‹ç›¸ä¼¼åº¦åˆ†æ•¸å’Œé—œéµè©åˆ†æ•¸
                return result.score * 0.7 + keyword_score * 0.3

            # é‡æ–°æ’åº
            results.sort(key=calculate_relevance_score, reverse=True)
            return results

        except Exception as e:
            logger.warning(f"çµæœé‡æ’åºå¤±æ•—: {e}")
            return results

    def _update_performance_stats(self, elapsed_time: float) -> None:
        """æ›´æ–°æ€§èƒ½çµ±è¨ˆä¿¡æ¯ ğŸ“ˆ"""
        total_queries = self._query_stats["total_queries"]
        prev_avg = self._query_stats["avg_response_time"]

        # è¨ˆç®—ç§»å‹•å¹³å‡
        self._query_stats["avg_response_time"] = (
            prev_avg * (total_queries - 1) + elapsed_time
        ) / total_queries

    def get_performance_stats(self) -> Dict:
        """ç²å–æ€§èƒ½çµ±è¨ˆä¿¡æ¯ ğŸ“Š"""
        stats = self._query_stats.copy()
        stats["cache_hit_rate"] = self._query_stats["cache_hits"] / max(
            self._query_stats["total_queries"], 1
        )
        stats["embedding_cache_hit_rate"] = self._query_stats[
            "embedding_cache_hits"
        ] / max(self._query_stats["total_queries"], 1)
        stats["active_cache_size"] = len(self._query_cache)
        stats["embedding_cache_size"] = len(self._embedding_cache)
        stats["uptime_seconds"] = time.time() - stats["last_reset_time"]

        return stats

    def reset_performance_stats(self) -> None:
        """é‡ç½®æ€§èƒ½çµ±è¨ˆ ğŸ”„"""
        self._query_stats = {
            "total_queries": 0,
            "cache_hits": 0,
            "embedding_cache_hits": 0,
            "avg_response_time": 0.0,
            "last_reset_time": time.time(),
        }
        logger.info("æ€§èƒ½çµ±è¨ˆå·²é‡ç½®")

    def get_document(self, doc_id: str) -> Optional[Dict]:
        """å–å¾—ç‰¹å®šæ–‡ä»¶å…§å®¹

        Args:
            doc_id: æ–‡ä»¶ID

        Returns:
            Optional[Dict]: æ–‡ä»¶å…§å®¹ï¼ŒåŒ…å« id, text, metadata
        """
        if not doc_id or not isinstance(doc_id, str):
            raise ValueError("doc_id must be a non-empty string")

        try:
            results = self.collection.get(
                ids=[doc_id], include=["documents", "metadatas"]
            )

            if results["ids"] and len(results["ids"]) > 0:
                return {
                    "id": results["ids"][0],
                    "text": results["documents"][0] if results["documents"] else "",
                    "metadata": results["metadatas"][0] if results["metadatas"] else {},
                }
            else:
                logger.warning(f"æ‰¾ä¸åˆ°æ–‡ä»¶ ID: {doc_id}")
                return None

        except Exception as e:
            logger.error(f"å–å¾—æ–‡ä»¶å¤±æ•— (ID: {doc_id}): {e}")
            return None

    def summarize_text(self, text: str, max_tokens: int = 120) -> str:
        """æ–‡æœ¬æ‘˜è¦åŠŸèƒ½ï¼ˆæ”¹é€²ç‰ˆï¼‰

        Args:
            text: åŸå§‹æ–‡æœ¬
            max_tokens: æœ€å¤§ token æ•¸é‡

        Returns:
            str: æ‘˜è¦æ–‡æœ¬
        """
        if not text or not isinstance(text, str):
            return ""

        if not isinstance(max_tokens, int) or max_tokens <= 0:
            raise ValueError("max_tokens must be a positive integer")

        # æŒ‰å¥å­åˆ†å‰²ä¸¦é™åˆ¶é•·åº¦
        sentences = [s.strip() for s in text.split("ã€‚") if s.strip()]
        if not sentences:
            return ""

        summary = ""
        token_count = 0

        for sentence in sentences:
            # ä¼°ç®— token æ•¸é‡ï¼ˆä¸­æ–‡å¤§ç´„ 1 å­—ç¬¦ = 1 tokenï¼‰
            estimated_tokens = len(sentence)

            if token_count + estimated_tokens > max_tokens:
                break

            summary += sentence + "ã€‚"
            token_count += estimated_tokens

        return summary.strip()

    def rebuild_index(self, path: str) -> Dict:
        """é‡å»ºç´¢å¼•ï¼ˆæ”¹é€²ç‰ˆï¼‰

        Args:
            path: è³‡æ–™è·¯å¾‘

        Returns:
            Dict: é‡å»ºçµæœç‹€æ…‹
        """
        if not path or not isinstance(path, str):
            raise ValueError("path must be a non-empty string")

        try:
            # ä½¿ç”¨é…ç½®ä¸­çš„ collection åç¨±
            collection_name = self.config.collection_name

            # é‡ç½® collection
            try:
                self.dbClient.delete_collection(collection_name)
            except Exception:
                logger.warning(f"Collection {collection_name} ä¸å­˜åœ¨ï¼Œè·³éåˆªé™¤")

            self.collection = self.dbClient.create_collection(collection_name)

            # æ¸…ç©ºç·©å­˜
            self._query_cache.clear()

            logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆ: {collection_name}")
            return {
                "status": "success",
                "details": f"ç´¢å¼•é‡å»ºå®Œæˆ: {collection_name}",
                "collection_name": collection_name,
            }

        except Exception as e:
            logger.error(f"é‡å»ºç´¢å¼•å¤±æ•—: {e}")
            return {
                "status": "error",
                "details": str(e),
                "collection_name": self.config.collection_name,
            }

    def get_collection_info(self) -> Dict:
        """å–å¾— collection è³‡è¨Šï¼ˆæ”¹é€²ç‰ˆï¼‰

        Returns:
            Dict: collection çµ±è¨ˆè³‡è¨Š
        """
        try:
            count = self.collection.count()
            return {
                "name": self.config.collection_name,
                "document_count": count,
                "status": "active",
                "cache_size": len(self._query_cache),
            }
        except Exception as e:
            logger.error(f"å–å¾— collection è³‡è¨Šå¤±æ•—: {e}")
            return {
                "name": self.config.collection_name,
                "document_count": 0,
                "status": "error",
                "error": str(e),
                "cache_size": 0,
            }

    def clear_cache(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰ç·©å­˜ ğŸ§¹"""
        self._query_cache.clear()
        self._embedding_cache.clear()
        self._preprocessed_queries.clear()
        logger.info("æ‰€æœ‰ç·©å­˜å·²æ¸…ç©ºï¼ˆæŸ¥è©¢ã€åµŒå…¥ã€é è™•ç†ï¼‰")

    def optimize_performance(self) -> Dict[str, str]:
        """æ€§èƒ½å„ªåŒ–å»ºè­° ğŸ’¡"""
        stats = self.get_performance_stats()
        suggestions = []

        if stats["cache_hit_rate"] < 0.3:
            suggestions.append("è€ƒæ…®å¢åŠ å¿«å–å¤§å°æˆ–å»¶é•· TTL")

        if stats["avg_response_time"] > 1.0:
            suggestions.append("éŸ¿æ‡‰æ™‚é–“è¼ƒæ…¢ï¼Œå»ºè­°æª¢æŸ¥ç¶²çµ¡é€£æ¥æˆ–æ¸›å°‘ top_k")

        if stats["embedding_cache_hit_rate"] < 0.2:
            suggestions.append("åµŒå…¥å¿«å–å‘½ä¸­ç‡ä½ï¼ŒæŸ¥è©¢æ¨¡å¼å¯èƒ½éæ–¼å¤šæ¨£åŒ–")

        if len(suggestions) == 0:
            suggestions.append("æ€§èƒ½è¡¨ç¾è‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥å„ªåŒ–")

        return {
            "performance_level": "good"
            if stats["avg_response_time"] < 0.5
            else "moderate"
            if stats["avg_response_time"] < 1.0
            else "poor",
            "suggestions": suggestions,
            "stats_summary": f"å¹³å‡éŸ¿æ‡‰æ™‚é–“: {stats['avg_response_time']:.3f}s, å¿«å–å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}",
        }


# å‘å¾Œå…¼å®¹çš„ä¾¿åˆ©å‡½æ•¸
def rag_search(query: str, top_k: int = 5) -> List[SearchResult]:
    """å¿«é€Ÿ RAG æœç´¢å‡½æ•¸ï¼ˆå‘å¾Œå…¼å®¹ï¼‰

    Args:
        query: æœç´¢æŸ¥è©¢
        top_k: è¿”å›çµæœæ•¸é‡

    Returns:
        List[SearchResult]: æœç´¢çµæœåˆ—è¡¨
    """
    rag_tools = RAGTools()
    return rag_tools.rag_search(query, top_k)


if __name__ == "__main__":
    rag_search("ä½ æœ‰ä»€éº¼æŠ€èƒ½ï¼Ÿ")
