"""ResumeMate é«˜æ€§èƒ½è™•ç†å™¨ ğŸš€

å”èª¿ Analysis Agent å’Œ Evaluate Agent çš„äº’å‹•ï¼Œä½¿ç”¨ OpenAI Agents SDK æ¨™æº–å¯¦ç¾
å…·å‚™æ€§èƒ½ç›£æ§ã€ç•°æ­¥è™•ç†ã€é€£æ¥æ± ç®¡ç†ç­‰å„ªåŒ–ç‰¹æ€§
"""

import asyncio
import time
import logging
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass, field

from backend.models import Question, SystemResponse
from backend.tools.rag import RAGTools
from backend.agents import AnalysisAgent, EvaluateAgent

logger = logging.getLogger(__name__)


@dataclass
class ProcessingStats:
    """è™•ç†æ€§èƒ½çµ±è¨ˆ"""

    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    avg_processing_time: float = 0.0
    min_processing_time: float = float("inf")
    max_processing_time: float = 0.0
    analysis_time: float = 0.0
    evaluation_time: float = 0.0
    concurrent_requests: int = 0
    cache_hits: int = 0
    start_time: float = field(default_factory=time.time)
    last_reset_time: float = field(default_factory=time.time)

    def update_timing(
        self,
        processing_time: float,
        analysis_time: float = 0.0,
        evaluation_time: float = 0.0,
    ):
        """æ›´æ–°æ™‚é–“çµ±è¨ˆ"""
        self.total_requests += 1

        # æ›´æ–°å¹³å‡è™•ç†æ™‚é–“
        prev_avg = self.avg_processing_time
        self.avg_processing_time = (
            prev_avg * (self.total_requests - 1) + processing_time
        ) / self.total_requests

        # æ›´æ–°æœ€å°å’Œæœ€å¤§æ™‚é–“
        self.min_processing_time = min(self.min_processing_time, processing_time)
        self.max_processing_time = max(self.max_processing_time, processing_time)

        # æ›´æ–°å„éšæ®µæ™‚é–“
        if analysis_time > 0:
            self.analysis_time = analysis_time
        if evaluation_time > 0:
            self.evaluation_time = evaluation_time

    def success(self):
        """è¨˜éŒ„æˆåŠŸè«‹æ±‚"""
        self.successful_requests += 1

    def failure(self):
        """è¨˜éŒ„å¤±æ•—è«‹æ±‚"""
        self.failed_requests += 1

    def get_success_rate(self) -> float:
        """ç²å–æˆåŠŸç‡"""
        if self.total_requests == 0:
            return 0.0
        return self.successful_requests / self.total_requests


class ResumeMateProcessor:
    """ResumeMate é«˜æ€§èƒ½ä¸»è™•ç†å™¨ ğŸš€"""

    def __init__(self, max_concurrent_requests: int = 10):
        """åˆå§‹åŒ–é«˜æ•ˆè™•ç†å™¨

        Args:
            max_concurrent_requests: æœ€å¤§ä¸¦ç™¼è«‹æ±‚æ•¸
        """
        # ğŸ”§ æ ¸å¿ƒçµ„ä»¶åˆå§‹åŒ–
        self.rag_tools = RAGTools()
        self.analysis_agent = AnalysisAgent()
        self.evaluate_agent = EvaluateAgent()

        # ğŸ“Š æ€§èƒ½ç›£æ§å’Œçµ±è¨ˆ
        self.stats = ProcessingStats()
        self.max_concurrent_requests = max_concurrent_requests
        self._request_semaphore = asyncio.Semaphore(max_concurrent_requests)

        # ğŸ’¾ ç°¡å–®è«‹æ±‚ç·©å­˜ (åŸºæ–¼å•é¡Œçš„é›œæ¹Šç·©å­˜)
        self._request_cache: Dict[str, Tuple[SystemResponse, float]] = {}
        self.enable_request_cache = True
        self.cache_ttl = 300  # 5 åˆ†é˜

        # ğŸš€ æ€§èƒ½å„ªåŒ–é…ç½®
        self.enable_parallel_processing = True
        self.request_timeout = 30.0  # 30 ç§’é€¾æ™‚

        logger.info("ğŸ† ResumeMate é«˜æ€§èƒ½è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(
            f"âš™ï¸  é…ç½®: æœ€å¤§ä¸¦ç™¼={max_concurrent_requests}, ç·©å­˜TTL={self.cache_ttl}s"
        )

    async def process_question(self, question: Question) -> SystemResponse:
        """é«˜æ•ˆä¸»è™•ç†æµç¨‹ ğŸ¯

        å…·å‚™ç¹å¿™æ§åˆ¶ã€ç·©å­˜æ©Ÿåˆ¶ã€æ€§èƒ½ç›£æ§å’Œç•°æ­¥è™•ç†å„ªåŒ–
        """
        start_time = time.time()
        request_id = f"{hash(question.text)}_{int(start_time * 1000) % 10000}"

        # ğŸ“Š æ›´æ–°ä¸¦ç™¼è¨ˆæ•¸å™¨
        self.stats.concurrent_requests += 1

        try:
            async with self._request_semaphore:  # ğŸšª ç¹å¿™æ§åˆ¶
                return await asyncio.wait_for(
                    self._process_question_internal(question, request_id, start_time),
                    timeout=self.request_timeout,
                )
        except asyncio.TimeoutError:
            logger.error(
                f"â° è«‹æ±‚é€¾æ™‚ ({self.request_timeout}s): {question.text[:50]}..."
            )
            self.stats.failure()
            return self._create_error_response("è«‹æ±‚è™•ç†é€¾æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
        except Exception as e:
            logger.error(f"âŒ è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            self.stats.failure()
            return self._create_error_response(f"è™•ç†éŒ¯èª¤: {str(e)}")
        finally:
            # ğŸ“Š æ›´æ–°çµ±è¨ˆè³‡è¨Š
            self.stats.concurrent_requests -= 1
            elapsed_time = time.time() - start_time
            self.stats.update_timing(elapsed_time)

    async def _process_question_internal(
        self, question: Question, request_id: str, start_time: float
    ) -> SystemResponse:
        """å…§éƒ¨è™•ç†é‚è¼¯ ğŸ”§"""
        logger.info(f"ğŸš€ [{request_id}] é–‹å§‹è™•ç†å•é¡Œ: {question.text[:50]}...")

        # ğŸ’¾ æª¢æŸ¥è«‹æ±‚ç·©å­˜
        if self.enable_request_cache:
            cache_response = self._check_request_cache(question.text)
            if cache_response:
                self.stats.cache_hits += 1
                self.stats.success()
                logger.info(f"âœ¨ [{request_id}] ç·©å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›")
                return cache_response

        analysis_start = time.time()

        try:
            # ğŸ” 1) åˆ†æéšæ®µ - ä½¿ç”¨é«˜æ•ˆç·©å­˜çš„ Analysis Agent
            analysis = await self.analysis_agent.analyze(question)
            analysis_time = time.time() - analysis_start

            # 1.1 å›å¡« sourcesï¼ˆç‚º Evaluate Agent æä¾›ä¾†æºä¿¡æ¯ï¼‰
            analysis.metadata = analysis.metadata or {}
            analysis.metadata.setdefault("sources", self._ensure_sources(analysis))
            analysis.metadata["request_id"] = request_id
            analysis.metadata["analysis_time"] = analysis_time

            logger.info(f"âœ… [{request_id}] åˆ†æå®Œæˆ ({analysis_time:.3f}s)")

            # ğŸ” 2) è©•ä¼°éšæ®µ - é€²è¡Œå“è³ªæª¢æ ¸èˆ‡å„ªåŒ–
            evaluation_start = time.time()

            if self.enable_parallel_processing:
                # ä¼µè¡Œè™•ç†ï¼šåŒæ™‚é€²è¡Œè©•ä¼°å’Œæ€§èƒ½çµ±è¨ˆ
                evaluation_task = asyncio.create_task(
                    self.evaluate_agent.evaluate(analysis)
                )
                stats_task = asyncio.create_task(
                    self._update_performance_metrics(analysis)
                )

                evaluation, _ = await asyncio.gather(
                    evaluation_task, stats_task, return_exceptions=True
                )

                if isinstance(evaluation, Exception):
                    raise evaluation
            else:
                evaluation = await self.evaluate_agent.evaluate(analysis)

            evaluation_time = time.time() - evaluation_start
            logger.info(f"âœ… [{request_id}] è©•ä¼°å®Œæˆ ({evaluation_time:.3f}s)")

            # ğŸ† 3) æ ¼å¼åŒ–æœ€çµ‚å›è¦†
            final_response = self._format_system_response(evaluation)

            # å¢åŠ æ€§èƒ½å…ƒæ•¸æ“š
            final_response.metadata.update(
                {
                    "request_id": request_id,
                    "processing_time": time.time() - start_time,
                    "analysis_time": analysis_time,
                    "evaluation_time": evaluation_time,
                    "cached": False,
                }
            )

            # ğŸ’¾ ç·©å­˜çµæœ
            if self.enable_request_cache:
                self._cache_response(question.text, final_response)

            self.stats.success()
            self.stats.update_timing(
                time.time() - start_time, analysis_time, evaluation_time
            )

            total_time = time.time() - start_time
            logger.info(f"âœ¨ [{request_id}] è™•ç†å®Œæˆ ({total_time:.3f}s)")

            return final_response

        except Exception as e:
            logger.error(f"âŒ [{request_id}] è™•ç†å¤±æ•—: {e}")
            self.stats.failure()
            raise

    # ğŸ”§ --------- å·¥å…·ï¼šæŠŠ retrievals/metadata è½‰ç‚º reviewer å¯ç”¨çš„ sources ----------
    def _ensure_sources(self, analysis) -> List[Dict[str, Any]]:
        sources: List[Dict[str, Any]] = []

        # (A) å¾ retrievals è½‰æˆå¯è¿½æº¯ä¾†æº
        try:
            if getattr(analysis, "retrievals", None):
                for r in analysis.retrievals:
                    md = getattr(r, "metadata", {}) or {}
                    sources.append(
                        {
                            "id": getattr(r, "doc_id", getattr(r, "id", None)),
                            "title": md.get("title"),
                            "loc": md.get("loc"),
                            "score": getattr(r, "score", None),
                        }
                    )
        except Exception as e:
            logger.warning(f"è½‰æ› retrievals ç‚º sources å¤±æ•—ï¼š{e}")

        # (B) è‹¥ analysis.metadata å·²æœ‰ sourcesï¼Œåˆä½µ
        try:
            meta_sources = (analysis.metadata or {}).get("sources", [])
            if isinstance(meta_sources, list):
                # è½‰æ› sources ç‚ºçµ±ä¸€æ ¼å¼
                for source in meta_sources:
                    if isinstance(source, str):
                        # å¦‚æœæ˜¯å­—ä¸² IDï¼Œè½‰ç‚ºå­—å…¸æ ¼å¼
                        sources.append(
                            {
                                "id": source,
                                "title": None,
                                "loc": None,
                                "score": 0.8,
                            }
                        )
                    elif isinstance(source, dict):
                        # å¦‚æœå·²ç¶“æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                        sources.append(source)
        except Exception as e:
            logger.warning(f"è½‰æ› metadata sources å¤±æ•—ï¼š{e}")

        # å»é‡ï¼ˆä»¥ id+locï¼‰
        seen = set()
        uniq: List[Dict[str, Any]] = []
        for s in sources:
            key = (s.get("id"), s.get("loc"))
            if key in seen:
                continue
            seen.add(key)
            uniq.append(s)

        return uniq[:5]  # æ§åˆ¶ä¸Šé™

    # ğŸ† --------- ç”¢å‡ºæœ€çµ‚ SystemResponseï¼ˆå«å‹•ä½œå»ºè­°ï¼‰ ----------
    def _format_system_response(self, evaluation) -> SystemResponse:
        """æ ¼å¼åŒ–ç‚ºç³»çµ±å›æ‡‰"""
        conf = max(0.0, min(1.0, float(getattr(evaluation, "confidence", 0.0))))

        # å…è¨±ä¸åŒæšèˆ‰/å­—ä¸²ï¼›çµ±ä¸€æ­¸ä¸€åŒ–
        status_str = str(getattr(evaluation.status, "value", evaluation.status)).lower()
        action = None

        if status_str in ("needs_clarification", "clarify"):
            action = "è«‹æä¾›æ›´å¤šè³‡è¨Š"
        elif status_str in ("out_of_scope", "oos"):
            # out_of_scope ä¹Ÿæ‡‰è©²å‡ç´šåˆ°äººå·¥è™•ç†
            action = "è«‹å¡«å¯«è¯çµ¡è¡¨å–®"
        elif status_str == "escalate_to_human":
            action = "è«‹å¡«å¯«è¯çµ¡è¡¨å–®"

        return SystemResponse(
            answer=evaluation.final_answer,
            sources=evaluation.sources or [],
            confidence=conf,
            action=action,
            metadata={
                "status": status_str,
                **(evaluation.metadata or {}),
            },
        )

    # ğŸ’» --------- ç³»çµ±è³‡è¨Šèˆ‡æ€§èƒ½ç›£æ§ ----------
    def _check_request_cache(self, question_text: str) -> Optional[SystemResponse]:
        """æª¢æŸ¥è«‹æ±‚ç·©å­˜ ğŸ’¾"""
        question_hash = str(hash(question_text.strip().lower()))

        if question_hash in self._request_cache:
            response, timestamp = self._request_cache[question_hash]
            if time.time() - timestamp < self.cache_ttl:
                # æ›´æ–°ç·©å­˜æ¨™è¨˜
                response.metadata["cached"] = True
                response.metadata["cache_age"] = time.time() - timestamp
                return response
            else:
                # æ¸…ç†éæœŸç·©å­˜
                del self._request_cache[question_hash]

        return None

    def _cache_response(self, question_text: str, response: SystemResponse) -> None:
        """ç·©å­˜å›æ‡‰çµæœ ğŸ’¾"""
        question_hash = str(hash(question_text.strip().lower()))
        current_time = time.time()

        # é™åˆ¶ç·©å­˜å¤§å°
        if len(self._request_cache) >= 100:
            # æ¸…ç†æœ€è€çš„ç·©å­˜é …ç›®
            oldest_key = min(
                self._request_cache.keys(), key=lambda k: self._request_cache[k][1]
            )
            del self._request_cache[oldest_key]

        self._request_cache[question_hash] = (response, current_time)

    def _create_error_response(self, error_message: str) -> SystemResponse:
        """å‰µå»ºéŒ¯èª¤å›æ‡‰ âŒ"""
        return SystemResponse(
            answer=f"æŠ±æ­‰ï¼Œ{error_message}",
            sources=[],
            confidence=0.0,
            action="è«‹ç¨å¾Œå†è©¦æˆ–è¯ç¹«æŠ€è¡“æ”¯æ´",
            metadata={
                "error": error_message,
                "timestamp": time.time(),
                "system_status": "error",
            },
        )

    async def _update_performance_metrics(self, analysis) -> None:
        """ç•°æ­¥æ›´æ–°æ€§èƒ½æŒ‡æ¨™ ğŸ“ˆ"""
        try:
            # æ›´æ–° RAG çµ±è¨ˆ
            if hasattr(self.rag_tools, "get_performance_stats"):
                rag_stats = self.rag_tools.get_performance_stats()
                logger.debug(
                    f"ğŸ“ˆ RAG æ€§èƒ½: å¹³å‡{rag_stats.get('avg_response_time', 0):.3f}s, "
                    f"ç·©å­˜å‘½ä¸­ç‡{rag_stats.get('cache_hit_rate', 0):.1%}"
                )
        except Exception as e:
            logger.debug(f"æ›´æ–°æ€§èƒ½æŒ‡æ¨™å¤±æ•—: {e}")

    def clear_cache(self) -> Dict[str, int]:
        """æ¸…ç†æ‰€æœ‰ç·©å­˜ ğŸ§¹"""
        request_cache_size = len(self._request_cache)
        self._request_cache.clear()

        # æ¸…ç† RAG ç·©å­˜
        rag_cache_cleared = 0
        if hasattr(self.rag_tools, "clear_cache"):
            try:
                rag_cache_info = self.rag_tools.get_performance_stats()
                rag_cache_cleared = rag_cache_info.get("active_cache_size", 0)
                self.rag_tools.clear_cache()
            except Exception as e:
                logger.warning(f"æ¸…ç† RAG ç·©å­˜å¤±æ•—: {e}")

        logger.info(
            f"ç·©å­˜å·²æ¸…ç†: è«‹æ±‚ç·©å­˜ {request_cache_size} å€‹, RAG ç·©å­˜ {rag_cache_cleared} å€‹"
        )

        return {
            "request_cache_cleared": request_cache_size,
            "rag_cache_cleared": rag_cache_cleared,
            "total_cleared": request_cache_size + rag_cache_cleared,
        }

    def get_performance_stats(self) -> Dict[str, Any]:
        """ç²å–è©³ç´°æ€§èƒ½çµ±è¨ˆ ğŸ“ˆ"""
        current_time = time.time()
        uptime = current_time - self.stats.start_time

        processor_stats = {
            "processor": {
                "total_requests": self.stats.total_requests,
                "successful_requests": self.stats.successful_requests,
                "failed_requests": self.stats.failed_requests,
                "success_rate": self.stats.get_success_rate(),
                "avg_processing_time": self.stats.avg_processing_time,
                "min_processing_time": self.stats.min_processing_time
                if self.stats.min_processing_time != float("inf")
                else 0.0,
                "max_processing_time": self.stats.max_processing_time,
                "current_concurrent_requests": self.stats.concurrent_requests,
                "cache_hits": self.stats.cache_hits,
                "cache_hit_rate": self.stats.cache_hits
                / max(self.stats.total_requests, 1),
                "uptime_seconds": uptime,
                "requests_per_second": self.stats.total_requests / max(uptime, 1),
            }
        }

        # å¢åŠ  RAG çµ±è¨ˆ
        try:
            if hasattr(self.rag_tools, "get_performance_stats"):
                processor_stats["rag"] = self.rag_tools.get_performance_stats()
        except Exception as e:
            logger.warning(f"ç²å– RAG çµ±è¨ˆå¤±æ•—: {e}")
            processor_stats["rag"] = {"error": str(e)}

        return processor_stats

    def get_system_info(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±è³‡è¨Š ğŸ’»"""
        db_info = {"document_count": 0, "status": "unknown"}
        if hasattr(self.rag_tools, "collection") and self.rag_tools.collection:
            try:
                count = self.rag_tools.collection.count()
                db_info["document_count"] = count
                db_info["status"] = "connected" if count > 0 else "empty"
            except Exception as e:
                logger.error(f"ç²å–æ•¸æ“šåº«è³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                db_info["status"] = "error"
                db_info["error"] = str(e)

        system_info = {
            "version": "2.0.0",
            "database": db_info,
            "performance": self.get_performance_stats(),
            "configuration": {
                "max_concurrent_requests": self.max_concurrent_requests,
                "request_timeout": self.request_timeout,
                "cache_enabled": self.enable_request_cache,
                "cache_ttl": self.cache_ttl,
                "parallel_processing": self.enable_parallel_processing,
            },
            "cache_status": {
                "request_cache_size": len(self._request_cache),
                "request_cache_limit": 100,
            },
        }

        return system_info

    def reset_stats(self) -> None:
        """é‡ç½®æ€§èƒ½çµ±è¨ˆ ğŸ”„"""
        self.stats = ProcessingStats()
        if hasattr(self.rag_tools, "reset_performance_stats"):
            self.rag_tools.reset_performance_stats()
        logger.info("æ€§èƒ½çµ±è¨ˆå·²é‡ç½®")

    async def health_check(self) -> Dict[str, Any]:
        """ç³»çµ±å¥åº·æª¢æŸ¥ â¤ï¸"""
        health_status = {
            "status": "healthy",
            "timestamp": time.time(),
            "components": {},
        }

        try:
            # æª¢æŸ¥è³‡æ–™åº«é€£æ¥
            if hasattr(self.rag_tools, "collection") and self.rag_tools.collection:
                doc_count = self.rag_tools.collection.count()
                health_status["components"]["database"] = {
                    "status": "healthy" if doc_count > 0 else "warning",
                    "document_count": doc_count,
                }
            else:
                health_status["components"]["database"] = {
                    "status": "unhealthy",
                    "message": "ç„¡æ³•é€£æ¥åˆ°è³‡æ–™åº«",
                }
        except Exception as e:
            health_status["components"]["database"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # æª¢æŸ¥ AI ä»£ç†
        try:
            health_status["components"]["ai_agents"] = {
                "analysis_agent": "healthy" if self.analysis_agent else "unhealthy",
                "evaluate_agent": "healthy" if self.evaluate_agent else "unhealthy",
            }
        except Exception as e:
            health_status["components"]["ai_agents"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # æª¢æŸ¥æ•´é«”ç³»çµ±ç‹€æ…‹
        if self.stats.get_success_rate() < 0.8 and self.stats.total_requests > 10:
            health_status["status"] = "degraded"
            health_status["message"] = (
                f"æˆåŠŸç‡éä½: {self.stats.get_success_rate():.1%}"
            )

        # æª¢æŸ¥ä¸¦ç™¼ç¨‹åº¦
        if self.stats.concurrent_requests >= self.max_concurrent_requests * 0.9:
            health_status["status"] = "warning"
            health_status["message"] = (
                f"é«˜ä¸¦ç™¼è² è¼‰: {self.stats.concurrent_requests}/{self.max_concurrent_requests}"
            )

        return health_status
